#!/usr/bin/env python3
"""
ç©ã¿ä¸Šã’å‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¤ã¤æ–°ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
- åŒä¸€é¦¬ã®è¤‡æ•°å›å‡ºå“ã«å¯¾å¿œã—ãŸå±¥æ­´ç®¡ç†
- è©³ç´°ãƒšãƒ¼ã‚¸URLå–å¾—ãƒ»ä¿æŒ
"""

import json
import os
import sys
import argparse
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import importlib.util

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(__file__))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'backend'))
sys.path.append(os.path.join(project_root, 'backend/scrapers'))

# æ¥½å¤©ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from backend.scrapers.rakuten_scraper import RakutenAuctionScraper
except ImportError:
    rakuten_scraper_path = os.path.join(project_root, 'backend/scrapers/rakuten_scraper.py')
    spec = importlib.util.spec_from_file_location('rakuten_scraper', rakuten_scraper_path)
    if spec is None or spec.loader is None:
        raise ImportError('rakuten_scraper.pyã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ')
    rakuten_scraper = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(rakuten_scraper)
    RakutenAuctionScraper = rakuten_scraper.RakutenAuctionScraper


class AccumulativeScraper:
    def __init__(self, enable_history=None, mode='development'):
        self.scraper = RakutenAuctionScraper()
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’ä½¿ç”¨
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        self.history_file = os.path.join(project_root, "static-frontend", "public", "data", "horses_history.json")
        
        # å±¥æ­´ç®¡ç†ã®åˆ¶å¾¡è¨­å®š
        self.mode = mode
        if enable_history is not None:
            self.enable_history = enable_history
        else:
            # ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ãƒ¢ãƒ¼ãƒ‰ã§åˆ¶å¾¡
            env_history = os.getenv('ENABLE_HISTORY_TRACKING', '').lower()
            if env_history in ['true', '1', 'yes']:
                self.enable_history = True
            elif env_history in ['false', '0', 'no']:
                self.enable_history = False
            else:
                # ãƒ¢ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®åˆ¶å¾¡
                self.enable_history = mode in ['production', 'prod']
        
        print(f"å±¥æ­´è¿½åŠ ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if self.enable_history else 'ç„¡åŠ¹'} (mode: {mode})")
        
    def load_existing_data(self) -> Dict:
        """æ—¢å­˜ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        if not os.path.exists(self.history_file):
            return {
                "metadata": {
                    "last_updated": datetime.now().isoformat(),
                    "total_horses": 0,
                    "average_price": 0
                },
                "horses": []
            }
        
        try:
            with open(self.history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return {
                "metadata": {
                    "last_updated": datetime.now().isoformat(),
                    "total_horses": 0,
                    "average_price": 0
                },
                "horses": []
            }
    
    def normalize_name(self, name: str) -> str:
        """é¦¬åã®æ­£è¦åŒ–ï¼ˆæ¯”è¼ƒç”¨ï¼‰"""
        if not name:
            return ""
        return name.strip().replace(" ", "").replace("ã€€", "")
    
    def find_matching_horse(self, new_horse: Dict, existing_horses: List[Dict]) -> Tuple[Optional[int], Optional[Dict]]:
        """
        åŒä¸€é¦¬ã‚’æ¤œç´¢
        åˆ¤å®šåŸºæº–: é¦¬å + è¡€çµ±æƒ…å ±ï¼ˆçˆ¶ã€æ¯ã€æ¯çˆ¶ï¼‰
        """
        new_name = self.normalize_name(new_horse.get('name', ''))
        new_sire = self.normalize_name(new_horse.get('sire', ''))
        new_dam = self.normalize_name(new_horse.get('dam', ''))
        new_dam_sire = self.normalize_name(new_horse.get('dam_sire', ''))
        
        for idx, existing_horse in enumerate(existing_horses):
            # é¦¬åã§ã®ä¸€è‡´ç¢ºèª
            existing_name = self.normalize_name(existing_horse.get('name', ''))
            if existing_name and new_name and existing_name == new_name:
                return idx, existing_horse
            
            # å±¥æ­´å†…ã®é¦¬åã‚‚ç¢ºèª
            for history_entry in existing_horse.get('history', []):
                history_name = self.normalize_name(history_entry.get('name', ''))
                if history_name and new_name and history_name == new_name:
                    return idx, existing_horse
            
            # è¡€çµ±æƒ…å ±ã§ã®ä¸€è‡´ç¢ºèªï¼ˆé¦¬åãŒç•°ãªã‚‹å ´åˆï¼‰
            existing_sire = self.normalize_name(existing_horse.get('sire', ''))
            existing_dam = self.normalize_name(existing_horse.get('dam', ''))
            existing_dam_sire = self.normalize_name(existing_horse.get('dam_sire', ''))
            
            if (new_sire and existing_sire and new_sire == existing_sire and
                new_dam and existing_dam and new_dam == existing_dam and
                new_dam_sire and existing_dam_sire and new_dam_sire == existing_dam_sire):
                return idx, existing_horse
        
        return None, None
    
    def create_history_entry(self, horse_data: Dict, auction_date: str) -> Dict:
        """å±¥æ­´ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ"""
        return {
            "auction_date": auction_date,
            "name": horse_data.get('name'),
            "sex": horse_data.get('sex'),
            "age": horse_data.get('age'),
            "seller": horse_data.get('seller'),
            "sold_price": horse_data.get('sold_price'),
            "start_price": horse_data.get('start_price'),
            "bid_num": horse_data.get('bid_num'),
            "unsold": horse_data.get('unsold', False),
            "comment": horse_data.get('comment', ''),
            "race_record": horse_data.get('race_record'),
            "total_prize_start": horse_data.get('total_prize_start'),
            "detail_url": horse_data.get('detail_url'),
            "primary_image": horse_data.get('primary_image'),
            "disease_tags": horse_data.get('disease_tags'),
            "weight": horse_data.get('weight')
        }
    
    def merge_horse_data(self, existing_horse: Dict, new_horse: Dict, auction_date: str) -> Dict:
        """æ—¢å­˜é¦¬ãƒ‡ãƒ¼ã‚¿ã«æ–°ã—ã„å±¥æ­´ã‚’è¿½åŠ """
        # æ–°ã—ã„å±¥æ­´ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ
        new_history_entry = self.create_history_entry(new_horse, auction_date)
        
        # å±¥æ­´é…åˆ—ã«è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
        if 'history' not in existing_horse:
            existing_horse['history'] = []
        
        # å±¥æ­´è¿½åŠ ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿å±¥æ­´ã‚’è¿½åŠ 
        if self.enable_history:
            # åŒã˜ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³æ—¥ã®å±¥æ­´ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            existing_dates = [h.get('auction_date') for h in existing_horse['history']]
            if auction_date not in existing_dates:
                existing_horse['history'].append(new_history_entry)
                print(f"  âœ… å±¥æ­´è¿½åŠ : {existing_horse.get('name', 'Unknown')} - {auction_date}")
            else:
                print(f"  âš ï¸  å±¥æ­´é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {existing_horse.get('name', 'Unknown')} - {auction_date}")
        else:
            print(f"  ğŸš« å±¥æ­´è¿½åŠ ã‚¹ã‚­ãƒƒãƒ—: {existing_horse.get('name', 'Unknown')} - {auction_date} (ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰)")
        
        # é¦¬å›ºæœ‰æƒ…å ±ã‚’æ›´æ–°ï¼ˆã‚ˆã‚Šæ–°ã—ã„æƒ…å ±ã§ä¸Šæ›¸ãï¼‰
        existing_horse.update({
            'updated_at': datetime.now().isoformat(),
            'sire': new_horse.get('sire') or existing_horse.get('sire'),
            'dam': new_horse.get('dam') or existing_horse.get('dam'),
            'dam_sire': new_horse.get('dam_sire') or existing_horse.get('dam_sire'),
            'jbis_url': new_horse.get('jbis_url') or existing_horse.get('jbis_url'),
            'netkeiba_url': new_horse.get('netkeiba_url') or existing_horse.get('netkeiba_url'),
        })
        
        # è©³ç´°ãƒšãƒ¼ã‚¸URLã‚’æ›´æ–°ï¼ˆæ–°ã—ã„ã‚‚ã®ãŒã‚ã‚Œã°ï¼‰
        if new_horse.get('detail_url'):
            existing_horse['detail_url'] = new_horse.get('detail_url')
        
        # æœ€æ–°ã®ä½“é‡ã‚’ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã«åæ˜ 
        latest_weight = self._get_latest_weight(existing_horse['history'])
        existing_horse['weight'] = latest_weight
        
        return existing_horse
    
    def clear_history_data(self, backup=True) -> bool:
        """å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ"""
        try:
            if not os.path.exists(self.history_file):
                print("âœ… å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                return True
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            if backup:
                backup_file = f"{self.history_file}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                import shutil
                shutil.copy2(self.history_file, backup_file)
                print(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_file}")
            
            # æ–°ã—ã„ç©ºã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ä½œæˆ
            empty_data = {
                "metadata": {
                    "last_updated": datetime.now().isoformat(),
                    "total_horses": 0,
                    "average_price": 0
                },
                "horses": []
            }
            
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(empty_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ: {self.history_file}")
            return True
            
        except Exception as e:
            print(f"âŒ å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªã‚¢ã«å¤±æ•—: {e}")
            return False
    
    def restore_from_backup(self, backup_file: str) -> bool:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ"""
        try:
            if not os.path.exists(backup_file):
                print(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {backup_file}")
                return False
            
            import shutil
            shutil.copy2(backup_file, self.history_file)
            print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒã—ã¾ã—ãŸ: {backup_file} -> {self.history_file}")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©å…ƒã«å¤±æ•—: {e}")
            return False
    
    def reset_history_count(self, backup=True, keep_latest_only=True, reset_mode='keep_latest', target_date=None) -> bool:
        """
        å±¥æ­´ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆã—ã€æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ã«å¿œã˜ã¦å±¥æ­´ã‚’ç®¡ç†
        
        Args:
            backup (bool): ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹ã‹
            keep_latest_only (bool): æœ€æ–°å±¥æ­´ã®ã¿ã‚’ä¿æŒã™ã‚‹ã‹ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ä¿æŒï¼‰
            reset_mode (str): ãƒªã‚»ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰
                - 'keep_latest': æœ€æ–°å±¥æ­´ã®ã¿ä¿æŒï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
                - 'remove_latest': æœ€æ–°å±¥æ­´ã®ã¿å‰Šé™¤
                - 'keep_oldest': æœ€å¤å±¥æ­´ã®ã¿ä¿æŒ
                - 'remove_oldest': æœ€å¤å±¥æ­´ã®ã¿å‰Šé™¤
                - 'keep_by_date': æŒ‡å®šæ—¥ä»˜ã®å±¥æ­´ã®ã¿ä¿æŒ
                - 'remove_by_date': æŒ‡å®šæ—¥ä»˜ã®å±¥æ­´ã®ã¿å‰Šé™¤
            target_date (str): æ—¥ä»˜æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ç”¨ã®å¯¾è±¡æ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
        """
        try:
            if not os.path.exists(self.history_file):
                print("âŒ å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                return False
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            if backup:
                backup_file = f"{self.history_file}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                import shutil
                shutil.copy2(self.history_file, backup_file)
                print(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_file}")
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            existing_data = self.load_existing_data()
            existing_horses = existing_data.get('horses', [])
            
            if not existing_horses:
                print("âŒ å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                return False
            
            # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®keep_latest_onlyã‚’reset_modeã«å¤‰æ›
            if keep_latest_only and reset_mode == 'keep_latest':
                reset_mode = 'keep_latest'
            elif not keep_latest_only and reset_mode == 'keep_latest':
                reset_mode = 'keep_all'  # å…¨å±¥æ­´ä¿æŒ
            
            reset_count = 0
            total_history_before = 0
            total_history_after = 0
            
            print(f"ğŸ”„ å±¥æ­´ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆã‚’é–‹å§‹: {len(existing_horses)}é ­ã®é¦¬ã‚’å‡¦ç†")
            print(f"ğŸ”§ ãƒªã‚»ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰: {reset_mode}")
            if target_date:
                print(f"ğŸ“… å¯¾è±¡æ—¥ä»˜: {target_date}")
            
            for horse in existing_horses:
                history = horse.get('history', [])
                total_history_before += len(history)
                original_count = len(history)
                
                if len(history) <= 1:
                    # å±¥æ­´ãŒ1ä»¶ä»¥ä¸‹ã®å ´åˆã¯å‡¦ç†ã—ãªã„
                    total_history_after += len(history)
                    continue
                
                new_history = self._filter_history_by_mode(history, reset_mode, target_date)
                
                if len(new_history) != original_count:
                    horse['history'] = new_history
                    reset_count += 1
                    total_history_after += len(new_history)
                    print(f"  âœ… {horse.get('name', 'Unknown')}: {original_count}ä»¶ -> {len(new_history)}ä»¶")
                else:
                    total_history_after += len(history)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
            updated_data = {
                "metadata": {
                    "last_updated": datetime.now().isoformat(),
                    "total_horses": len(existing_horses),
                    "average_price": existing_data.get('metadata', {}).get('average_price', 0),
                    "auction_date": datetime.now().strftime('%Y-%m-%d'),
                    "history_reset": True,
                    "reset_date": datetime.now().isoformat(),
                    "horses_reset": reset_count,
                    "total_history_before": total_history_before,
                    "total_history_after": total_history_after
                },
                "horses": existing_horses
            }
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(updated_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… å±¥æ­´ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")
            print(f"  ãƒªã‚»ãƒƒãƒˆå¯¾è±¡: {reset_count}é ­")
            print(f"  å±¥æ­´æ•°: {total_history_before}ä»¶ -> {total_history_after}ä»¶")
            print(f"  ä¿å­˜å…ˆ: {self.history_file}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å±¥æ­´ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆã«å¤±æ•—: {e}")
            return False
    
    def _filter_history_by_mode(self, history: List[Dict], reset_mode: str, target_date: str = None) -> List[Dict]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦å±¥æ­´ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        
        Args:
            history (List[Dict]): å…ƒã®å±¥æ­´ãƒªã‚¹ãƒˆ
            reset_mode (str): ãƒªã‚»ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰
            target_date (str): æ—¥ä»˜æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ç”¨ã®å¯¾è±¡æ—¥ä»˜
            
        Returns:
            List[Dict]: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®å±¥æ­´ãƒªã‚¹ãƒˆ
        """
        if not history:
            return history
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆã—ãŸå±¥æ­´ã‚’ä½œæˆ
        sorted_history = sorted(history, key=lambda x: x.get('auction_date', ''))
        
        if reset_mode == 'keep_latest':
            # æœ€æ–°ã®å±¥æ­´ã®ã¿ä¿æŒ
            return [sorted_history[-1]] if sorted_history else []
        
        elif reset_mode == 'remove_latest':
            # æœ€æ–°ã®å±¥æ­´ã®ã¿å‰Šé™¤
            return sorted_history[:-1] if len(sorted_history) > 1 else []
        
        elif reset_mode == 'keep_oldest':
            # æœ€å¤ã®å±¥æ­´ã®ã¿ä¿æŒ
            return [sorted_history[0]] if sorted_history else []
        
        elif reset_mode == 'remove_oldest':
            # æœ€å¤ã®å±¥æ­´ã®ã¿å‰Šé™¤
            return sorted_history[1:] if len(sorted_history) > 1 else []
        
        elif reset_mode == 'keep_by_date':
            # æŒ‡å®šæ—¥ä»˜ã®å±¥æ­´ã®ã¿ä¿æŒ
            if not target_date:
                print(f"âš ï¸  æ—¥ä»˜æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ã§ã™ãŒå¯¾è±¡æ—¥ä»˜ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return history
            return [h for h in history if h.get('auction_date') == target_date]
        
        elif reset_mode == 'remove_by_date':
            # æŒ‡å®šæ—¥ä»˜ã®å±¥æ­´ã®ã¿å‰Šé™¤
            if not target_date:
                print(f"âš ï¸  æ—¥ä»˜æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ã§ã™ãŒå¯¾è±¡æ—¥ä»˜ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return history
            return [h for h in history if h.get('auction_date') != target_date]
        
        elif reset_mode == 'keep_all':
            # å…¨å±¥æ­´ä¿æŒï¼ˆãƒªã‚»ãƒƒãƒˆã—ãªã„ï¼‰
            return history
        
        else:
            print(f"âš ï¸  ä¸æ˜ãªãƒªã‚»ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰: {reset_mode}")
            return history
    
    def _get_latest_weight(self, history: List[Dict]) -> Optional[int]:
        """å±¥æ­´ã‹ã‚‰æœ€æ–°ã®ä½“é‡ã‚’å–å¾—"""
        if not history:
            return None
        
        # å±¥æ­´ã‚’æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæœ€æ–°ãŒæœ€å¾Œï¼‰
        sorted_history = sorted(history, key=lambda x: x.get('auction_date', ''))
        
        # æœ€æ–°ã®å±¥æ­´ã‹ã‚‰ä½“é‡ã‚’å–å¾—
        for entry in reversed(sorted_history):
            weight = entry.get('weight')
            if weight is not None:
                return weight
        
        return None
    
    def create_new_horse_entry(self, horse_data: Dict, auction_date: str, horse_id: int) -> Dict:
        """æ–°ã—ã„é¦¬ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ"""
        history_entry = self.create_history_entry(horse_data, auction_date)
        
        return {
            'id': horse_id,
            'name': horse_data.get('name'),
            'sire': horse_data.get('sire'),
            'dam': horse_data.get('dam'),
            'dam_sire': horse_data.get('dam_sire'),
            'jbis_url': horse_data.get('jbis_url'),
            'netkeiba_url': horse_data.get('netkeiba_url'),
            'detail_url': horse_data.get('detail_url'),
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat(),
            'history': [history_entry],
            'weight': horse_data.get('weight')  # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã«ä½“é‡ã‚’è¿½åŠ 
        }
    
    def scrape_and_accumulate(self) -> bool:
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œï¼‹ãƒ‡ãƒ¼ã‚¿ç©ã¿ä¸Šã’"""
        print("=== ç©ã¿ä¸Šã’å‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ ===")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        existing_data = self.load_existing_data()
        existing_horses = existing_data.get('horses', [])
        
        print(f"æ—¢å­˜é¦¬ãƒ‡ãƒ¼ã‚¿: {len(existing_horses)}é ­")
        
        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        print("æ–°ã—ã„ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        new_horses = self.scraper.scrape_all_horses()
        
        if not new_horses:
            print("æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return False
        
        print(f"æ–°è¦å–å¾—: {len(new_horses)}é ­")
        
        # ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³æ—¥ã‚’å–å¾—
        auction_date = self.scraper.get_auction_date()
        
        # ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†
        added_count = 0
        updated_count = 0
        next_id = max([h.get('id', 0) for h in existing_horses], default=0) + 1
        
        for new_horse in new_horses:
            # åŒä¸€é¦¬ã‚’æ¤œç´¢
            match_idx, existing_horse = self.find_matching_horse(new_horse, existing_horses)
            
            if existing_horse is not None:
                # æ—¢å­˜é¦¬ã®å±¥æ­´ã‚’æ›´æ–°
                print(f"å±¥æ­´æ›´æ–°: {new_horse.get('name')} (ID: {existing_horse.get('id')})")
                updated_horse = self.merge_horse_data(existing_horse, new_horse, auction_date)
                existing_horses[match_idx] = updated_horse
                updated_count += 1
            else:
                # æ–°ã—ã„é¦¬ã¨ã—ã¦è¿½åŠ 
                print(f"æ–°è¦è¿½åŠ : {new_horse.get('name')} (ID: {next_id})")
                new_entry = self.create_new_horse_entry(new_horse, auction_date, next_id)
                existing_horses.append(new_entry)
                next_id += 1
                added_count += 1
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        total_horses = len(existing_horses)
        all_prices = []
        for horse in existing_horses:
            for history in horse.get('history', []):
                price = history.get('sold_price')
                if price and not history.get('unsold', False):
                    all_prices.append(price)
        
        avg_price = sum(all_prices) / len(all_prices) if all_prices else 0
        
        updated_data = {
            "metadata": {
                "last_updated": datetime.now().isoformat(),
                "total_horses": total_horses,
                "average_price": int(avg_price),
                "auction_date": auction_date,
                "added_horses": added_count,
                "updated_horses": updated_count
            },
            "horses": existing_horses
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        print(f"ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {os.path.dirname(self.history_file)}")
        os.makedirs(os.path.dirname(self.history_file), exist_ok=True)
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–‹å§‹: {self.history_file}")
        print(f"ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: é¦¬æ•°={len(existing_horses)}, ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿={updated_data['metadata']}")
        
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(updated_data, f, ensure_ascii=False, indent=2)
        
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {self.history_file}")
        
        # ä¿å­˜ç¢ºèª
        if os.path.exists(self.history_file):
            file_size = os.path.getsize(self.history_file)
            print(f"ä¿å­˜ç¢ºèªOK: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º={file_size}ãƒã‚¤ãƒˆ")
        else:
            print(f"[ã‚¨ãƒ©ãƒ¼] ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—: {self.history_file}")
        
        print(f"=== ç©ã¿ä¸Šã’å®Œäº† ===")
        print(f"æ–°è¦è¿½åŠ : {added_count}é ­")
        print(f"å±¥æ­´æ›´æ–°: {updated_count}é ­")
        print(f"ç·é¦¬æ•°: {total_horses}é ­")
        print(f"ä¿å­˜å…ˆ: {self.history_file}")
        
        return True


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='ç©ã¿ä¸Šã’å‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ï¼ˆå±¥æ­´è¿½åŠ æœ‰åŠ¹ï¼‰
  python3 accumulative_scraper.py --mode production
  
  # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆå±¥æ­´è¿½åŠ ç„¡åŠ¹ï¼‰
  python3 accumulative_scraper.py --mode test
  
  # ç’°å¢ƒå¤‰æ•°ã§åˆ¶å¾¡
  ENABLE_HISTORY_TRACKING=true python3 accumulative_scraper.py
  
  # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä»˜ãï¼‰
  python3 accumulative_scraper.py --clear-data
  
  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ
  python3 accumulative_scraper.py --restore backup_file.json
  
  # å±¥æ­´ãƒªã‚»ãƒƒãƒˆä¾‹:
  # æœ€æ–°å±¥æ­´ã®ã¿ä¿æŒï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
  python3 accumulative_scraper.py --reset-history
  
  # æœ€æ–°å±¥æ­´ã®ã¿å‰Šé™¤ï¼ˆå¤ã„ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿æŒï¼‰
  python3 accumulative_scraper.py --reset-history --reset-mode remove_latest
  
  # æœ€å¤å±¥æ­´ã®ã¿ä¿æŒ
  python3 accumulative_scraper.py --reset-history --reset-mode keep_oldest
  
  # æœ€å¤å±¥æ­´ã®ã¿å‰Šé™¤
  python3 accumulative_scraper.py --reset-history --reset-mode remove_oldest
  
  # ç‰¹å®šæ—¥ä»˜ã®å±¥æ­´ã®ã¿ä¿æŒ
  python3 accumulative_scraper.py --reset-history --reset-mode keep_by_date --target-date 2025-07-25
  
  # ç‰¹å®šæ—¥ä»˜ã®å±¥æ­´ã®ã¿å‰Šé™¤
  python3 accumulative_scraper.py --reset-history --reset-mode remove_by_date --target-date 2025-07-25
        """
    )
    
    parser.add_argument(
        '--mode', 
        choices=['production', 'prod', 'development', 'dev', 'test'],
        default='development',
        help='å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ (production: å±¥æ­´è¿½åŠ æœ‰åŠ¹, test/development: å±¥æ­´è¿½åŠ ç„¡åŠ¹)'
    )
    
    parser.add_argument(
        '--enable-history',
        action='store_true',
        help='å±¥æ­´è¿½åŠ ã‚’å¼·åˆ¶çš„ã«æœ‰åŠ¹åŒ–'
    )
    
    parser.add_argument(
        '--disable-history',
        action='store_true', 
        help='å±¥æ­´è¿½åŠ ã‚’å¼·åˆ¶çš„ã«ç„¡åŠ¹åŒ–'
    )
    
    parser.add_argument(
        '--clear-data',
        action='store_true',
        help='å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼‰'
    )
    
    parser.add_argument(
        '--restore',
        metavar='BACKUP_FILE',
        help='ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ'
    )
    
    parser.add_argument(
        '--no-backup',
        action='store_true',
        help='ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢æ™‚ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆã—ãªã„'
    )
    
    parser.add_argument(
        '--reset-history',
        action='store_true',
        help='å±¥æ­´ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆæœ€æ–°å±¥æ­´ã®ã¿ä¿æŒï¼‰'
    )
    
    parser.add_argument(
        '--keep-all-history',
        action='store_true',
        help='å±¥æ­´ãƒªã‚»ãƒƒãƒˆæ™‚ã«å…¨å±¥æ­´ã‚’ä¿æŒï¼ˆã‚«ã‚¦ãƒ³ãƒˆã®ã¿ãƒªã‚»ãƒƒãƒˆï¼‰'
    )
    
    parser.add_argument(
        '--reset-mode',
        choices=['keep_latest', 'remove_latest', 'keep_oldest', 'remove_oldest', 'keep_by_date', 'remove_by_date'],
        default='keep_latest',
        help='å±¥æ­´ãƒªã‚»ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã‚’æŒ‡å®š'
    )
    
    parser.add_argument(
        '--target-date',
        metavar='YYYY-MM-DD',
        help='æ—¥ä»˜æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ç”¨ã®å¯¾è±¡æ—¥ä»˜ï¼ˆkeep_by_date/remove_by_dateã§ä½¿ç”¨ï¼‰'
    )
    
    args = parser.parse_args()
    
    # å±¥æ­´è¿½åŠ ã®åˆ¶å¾¡è¨­å®š
    enable_history = None
    if args.enable_history:
        enable_history = True
    elif args.disable_history:
        enable_history = False
    
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    scraper = AccumulativeScraper(enable_history=enable_history, mode=args.mode)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢å‡¦ç†
    if args.clear_data:
        print("ğŸ—‘ï¸  å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªã‚¢ã‚’å®Ÿè¡Œä¸­...")
        success = scraper.clear_history_data(backup=not args.no_backup)
        if success:
            print("âœ… å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªã‚¢ãŒå®Œäº†ã—ã¾ã—ãŸ")
        else:
            print("âŒ å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªã‚¢ã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
        return
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒå‡¦ç†
    if args.restore:
        print(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©å…ƒã‚’å®Ÿè¡Œä¸­: {args.restore}")
        success = scraper.restore_from_backup(args.restore)
        if success:
            print("âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©å…ƒãŒå®Œäº†ã—ã¾ã—ãŸ")
        else:
            print("âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©å…ƒã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
        return
    
    # å±¥æ­´ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆå‡¦ç†
    if args.reset_history:
        print("ğŸ”„ å±¥æ­´ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆã‚’å®Ÿè¡Œä¸­...")
        
        # ãƒªã‚»ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã®æ±ºå®š
        reset_mode = args.reset_mode
        if args.keep_all_history:
            reset_mode = 'keep_all'
        
        # æ—¥ä»˜æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if reset_mode in ['keep_by_date', 'remove_by_date'] and not args.target_date:
            print("âŒ æ—¥ä»˜æŒ‡å®šãƒ¢ãƒ¼ãƒ‰ã§ã¯ --target-date ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã™")
            sys.exit(1)
        
        # æ—¥ä»˜å½¢å¼ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if args.target_date:
            import re
            if not re.match(r'\d{4}-\d{2}-\d{2}', args.target_date):
                print("âŒ æ—¥ä»˜ã¯ YYYY-MM-DD å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„")
                sys.exit(1)
        
        # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®keep_latest_onlyã‚’è¨­å®š
        keep_latest_only = (reset_mode == 'keep_latest')
        
        success = scraper.reset_history_count(
            backup=not args.no_backup, 
            keep_latest_only=keep_latest_only,
            reset_mode=reset_mode,
            target_date=args.target_date
        )
        
        if success:
            mode_descriptions = {
                'keep_latest': 'æœ€æ–°å±¥æ­´ã®ã¿ä¿æŒ',
                'remove_latest': 'æœ€æ–°å±¥æ­´ã®ã¿å‰Šé™¤',
                'keep_oldest': 'æœ€å¤å±¥æ­´ã®ã¿ä¿æŒ',
                'remove_oldest': 'æœ€å¤å±¥æ­´ã®ã¿å‰Šé™¤',
                'keep_by_date': f'{args.target_date}ã®å±¥æ­´ã®ã¿ä¿æŒ',
                'remove_by_date': f'{args.target_date}ã®å±¥æ­´ã®ã¿å‰Šé™¤',
                'keep_all': 'å…¨å±¥æ­´ä¿æŒ'
            }
            description = mode_descriptions.get(reset_mode, reset_mode)
            print(f"âœ… å±¥æ­´ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆï¼ˆ{description}ï¼‰ãŒå®Œäº†ã—ã¾ã—ãŸ")
        else:
            print("âŒ å±¥æ­´ã‚«ã‚¦ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
        return
    
    # é€šå¸¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†
    success = scraper.scrape_and_accumulate()
    
    if success:
        print("âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‹ç©ã¿ä¸Šã’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
    else:
        print("âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)


if __name__ == "__main__":
    main()
